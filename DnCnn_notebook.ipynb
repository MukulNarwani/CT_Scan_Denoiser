{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "DnCnn_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MukulNarwani/CT_Scan_Denoiser/blob/main/DnCnn_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v704GmF2PZNC"
      },
      "source": [
        "For this assignment I ended up making a DnCNN (De-noising convolutional neural network) to denoise the dataset. I explain how I arrived to making a DnCNN in a later section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0DzGWWD0wOy"
      },
      "source": [
        "from keras.models import Model,Sequential\n",
        "from tensorflow.keras import  Input\n",
        "from keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt\n",
        "import skimage.io as skio\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib \n",
        "from google.colab import drive\n",
        "from keras.models import *\n",
        "from keras.layers import  Input,Conv2D,BatchNormalization,Activation,Lambda,Subtract\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOYCVi23DEeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d0b1b7-8723-4691-db50-34b8845e26de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEYW3KQPPbim"
      },
      "source": [
        "This model uses a deep convolutional network for denoising. It takes in an input of dimensions (512,512,1) and applies a convolution with 64 filters and a kernel size of 3\n",
        "\n",
        "The next 10 layers are conv layers with 64 filters and a kernel size of 3. They use batch normalization and have a relu activation layer. \n",
        "\n",
        "The last conv layer has one filter to recreate the image with a kernel size of 3. \n",
        "\n",
        "The model was meant to capture the residual image of the noise, and have that subtracted from the input image. I wanted to divide the input image into quarters of shape (256,256,1), this would have given the model more data to work with. but this was something I didn't get a chance to implement. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQfbdXcL1DoV"
      },
      "source": [
        "def DnCNN():\n",
        "    \n",
        "    inpt = Input(shape=(512,512,1))\n",
        "    # First layer\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(inpt)\n",
        "    x = Activation('relu')(x)\n",
        "    # 10 layers, Conv+BN+relu\n",
        "    for i in range(10):\n",
        "        x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
        "        x = BatchNormalization(axis=-1, epsilon=1e-3)(x)\n",
        "        x = Activation('relu')(x)   \n",
        "    # last layer, Conv\n",
        "    x = Conv2D(filters=1, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
        "    x = Subtract()([inpt, x])   # input - noise\n",
        "    model = Model(inputs=inpt, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6fEo7SaEB3U"
      },
      "source": [
        "chckpoints = '/content/drive/MyDrive/Colab Notebooks/checkpointsv2/'\n",
        "checkpoint_callback=ModelCheckpoint(filepath=chckpoints+'weights.{epoch:02d}.hdf5', save_weights_only=True,verbose=1)\n",
        "tensorboard_callback=TensorBoard()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksZikMF8LIi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d3385c-6801-448c-bc80-fcca6e302753"
      },
      "source": [
        "import os \n",
        "print(os.path.exists('/content/drive/MyDrive/Colab Notebooks/checkpointsv2/'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFz5wqwJg_oD"
      },
      "source": [
        "I import all the tif datasets here and load them into memory. I split the dataset into 400-Training sets and 112-Testing sets, which is ~78%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "totn7___0wOz"
      },
      "source": [
        "\n",
        "clean_images = skio.imread(\"/content/drive/MyDrive/Colab Notebooks/fly_VNC.tif\", plugin=\"tifffile\")\n",
        "noisy_images = skio.imread(\"/content/drive/MyDrive/Colab Notebooks/fly_VNC_4x_subsampled.tif\", plugin=\"tifffile\")\n",
        "\n",
        "mouse_cortex_noisy=skio.imread(\"/content/drive/MyDrive/Colab Notebooks/mouse_cortex.tif\", plugin=\"tifffile\")\n",
        "mouse_cortex_clean=skio.imread(\"/content/drive/MyDrive/Colab Notebooks/mouse_cortex_4x_subsampled.tif\", plugin=\"tifffile\")\n",
        "\n",
        "# For this assignment I created batches of size 5 because I was running into a lot of memory issues.\n",
        "# And batches of size 5 were the highest size that seemed to worked\n",
        "\n",
        "training_set = tf.data.Dataset.from_tensor_slices((noisy_images[:400,:,:],clean_images[:400,:,:])).batch(5)\n",
        "testing_set = tf.data.Dataset.from_tensor_slices((noisy_images[400:,:,:],clean_images[400:,:,:])).batch(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJN05UxAhTvA"
      },
      "source": [
        "DnCnn = DnCNN()\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "DnCnn.compile(optimizer=optimizer,loss='mean_squared_error',metrics=['accuracy'])\n",
        "# DnCnn.load_weights('/content/drive/MyDrive/Colab Notebooks/HMS_code_test_xray/checkpoints/weights.25.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcA_-csz0wO0"
      },
      "source": [
        "history = DnCnn.fit(training_set,epochs = 15, validation_data=testing_set,callbacks=[checkpoint_callback,tensorboard_callback])\n",
        "plt.plot(history.history['accuracy'],label='accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeh9Z5DUKhFZ"
      },
      "source": [
        "from tifffile import imsave\n",
        "\n",
        "# There was a weird bug, that wouldn't let me show the images unless \n",
        "# I specified the dimensions like below\n",
        "\n",
        "# You can test the trained network by changing the dimensions \n",
        "# to view any slice\n",
        "\n",
        "\n",
        "pred=secondDnCnn.predict(mouse_cortex_noisy[340:341,:,:])\n",
        "plt.imshow(pred[0,:,:,0],cmap='gray')\n",
        "imsave('/content/drive/MyDrive/Colab Notebooks/output.tif',pred)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwKhZzX5hxlr"
      },
      "source": [
        "I trained the network for 15 epochs (which took about 30 minutes), on the 4x_subsampled fly VNC (the noisy VNC image). I passed in a random unseen slice to the trained network and show the output below. The image on the right is the output from the network. \n",
        "\n",
        "The network is succesfully learning the noise representation and is denoising the input image, but in doing so also reduces the quality of the image. I suspect this is because of the loss function I used. According to [1] using SSIM for the loss function instead of MSE, would yield a better quality image and improve training times. I would like to rerun this experiment with PSNR and SSIM loss functions. I also suspect reducing the input dimension might improve results as the network would have a smaller dynamic range to focus on. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6xZ2vJliz_i"
      },
      "source": [
        "<img src =\"https://drive.google.com/uc?export=view&id=1V-3ODN0_Nt_YUdelG9q5BlWImrI0V4b4\" width=\"500\">          <img src =\"https://drive.google.com/uc?export=view&id=1MZArnCJi0wQGmWdI_JogAwTGcyU-P1sp\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMm32hQ1nRVt"
      },
      "source": [
        "I then ran the network trained on the fly VNC on the mouse cortex, results are consistent across domains, but the drawbacks are worse in the cortex. Since this part of the cortext doesn't have as high of a resolution to begin with, any resolution loss is very detriental to quality. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmAWpEmLnLJ_"
      },
      "source": [
        "\n",
        "<img src =\"https://drive.google.com/uc?export=view&id=1-6019Hh5WMa3n71S9TDocanAgnnWzmOT\" width=\"500\">\n",
        "\n",
        "<img src =\"https://drive.google.com/uc?export=view&id=1MlasRyiUGgS23vzlKqXykCXUE0qVAOhl\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLzcZd1DZFFO"
      },
      "source": [
        "When I got the assignment, I read some research papers on networks that would work best for this task. I found that many recent papers were using GAN architectures for denoising. I've been meaning to learn how GANs are implemented so decided to use this as an oppurtunity to learn and experiment with one. \n",
        "\n",
        "I was closely following the '*Low-dose CT Image Restoration using generative adversarial networks*' (J. Kim) paper [1]. In the paper they use a DnCNN for the generator and a VGG16 trained as a binary classifier for the discriminator. I wrote the model architecture for both the discrimiantor and the generator but ran into many roadblocks along the way while attempting to train the network. Finally I decided to just use the DnCNN network and train that to denoise the dataset. From what I understand, training a GAN is much more effective at denoising. Another model that I was planning on testing out is '*TomoGAN: low-dose synchrotron x-ray tomography with generative adversarial networks*' (Zhengchun Liu, et. al.). They use an autoencoder style U-Net to model the the noise vector. It seemed like they used a relatively shallow CNN for the discriminator and achieved quite succesful results. \n",
        "\n",
        "All the papers that I read used leaky Relu for their activation functions, I am curious to how that would affect the results of my current network. \n",
        "\n",
        "If I had more time, I would use this DnCNN and try to get better results by using different loss functions, I'm quite sure that would show the greatest potential for change. Once I have that down I would like to compare how a TomoGan or a normal DcGAN would compete with just the base DnCNN. \n",
        "\n",
        "Using PSNR or SSIM seems like the industry standard in determining the quality of the network. Understanding how they work would be crucial in crafting the right loss function for the model. \n",
        "\n",
        "I am curious to see how running the network with deeper layers affects the output, and if I were to run this experiment again with a larger dataset I would experiment with constructing deeper/wider CNNs. While training the model, it does not seem like it is prone to overfitting, but using the MSE loss function is unreliable and I don't think it accurately captures the intricacies of this problem."
      ]
    }
  ]
}